{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import json # for reading from json file\n",
    "import glob # for listing files inside a folder\n",
    "from PIL import Image, ImageDraw # for reading images and drawing masks on them.\n",
    "\n",
    "\n",
    "# Create a custom dataset class for Nature\n",
    "# dataset subclassing PyTorch's Dataset class\n",
    "class PrimitiveShapesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root1,root2,transforms,mask_transforms):\n",
    "        self.transforms = transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "        self.imgs = glob.glob(root1 + '/*.png')\n",
    "        \n",
    "        self.root1 = root1\n",
    "        self.root2 = root2\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Have already aligned images and JSON files; can now\n",
    "        # simply use the index to access both images and masks\n",
    "        img_path = self.imgs[idx]\n",
    "        \n",
    "        mask_path = self.root2 +\"/\" + self.imgs[idx].split(\"/\")[-1]\n",
    "        \n",
    "        # Set the desired padding width\n",
    "        padding_width = 16\n",
    "\n",
    "        # Create a new image with the desired width and the same height as the original image\n",
    "        new_image = Image.new('RGB', (240 + padding_width, 160))\n",
    "\n",
    "        # Paste the original image onto the new image\n",
    "        new_image.paste(Image.open(img_path), (0, 0))\n",
    "\n",
    "        \n",
    "        \n",
    "        # Read image using PIL.Image and convert it to an RGB image\n",
    "        img = np.moveaxis(np.array(new_image) ,  -1, 0)\n",
    "        \n",
    "        \n",
    "        # Set the desired padding width\n",
    "        mask_padding_width = 16\n",
    "\n",
    "        # Create a new image with the desired width and the same height as the original image\n",
    "        mask_new_image = Image.new('L', (240+mask_padding_width, 160))\n",
    "        \n",
    "#         print(np.unique(Image.open(mask_path)))\n",
    "\n",
    "        # Paste the original image onto the new image\n",
    "        mask_new_image.paste(Image.open(mask_path), (0, 0))\n",
    "        \n",
    "        mask_img = np.array(mask_new_image)\n",
    "        \n",
    "#         print(\"tester\",mask_img.max())\n",
    "        \n",
    "        all_classes = np.unique(mask_img)\n",
    "        \n",
    "        sample = dict(image=img, mask=mask_img)\n",
    "        # Apply transforms\n",
    "#         if self.transforms is not None:\n",
    "#             img = self.transforms(img)\n",
    "#             mask_img = torch.tensor(np.array(self.mask_transforms(mask_img)))\n",
    "            \n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# init train, val, test sets\n",
    "train_dataset = PrimitiveShapesDataset(\"Dataset_Student/train_flat\", \"Dataset_Student/train_targets_flat\",None,None)\n",
    "valid_dataset = PrimitiveShapesDataset(\"Dataset_Student/val_flat\", \"Dataset_Student/val_targets_flat\",None,None)\n",
    "\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Valid size: {len(valid_dataset)}\")\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28532798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arch, encoder_name,encoder_weights, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, encoder_weights= encoder_weights, in_channels=in_channels, classes=out_classes, **kwargs, \n",
    "        )\n",
    "\n",
    "        # preprocessing parameteres for image\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name, pretrained=\"customnet\")\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
    "        \n",
    "        self.softmaxfunc = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.metrics = torchmetrics.IoU(num_classes=out_classes)\n",
    "        \n",
    "        self.no_of_classes = out_classes\n",
    "\n",
    "    def forward(self, image):\n",
    "        image = image/255.0\n",
    "        # normalize image here\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        \n",
    "        image = batch[\"image\"]\n",
    "\n",
    "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
    "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
    "        assert image.ndim == 4\n",
    "\n",
    "        # Check that image dimensions are divisible by 32, \n",
    "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n",
    "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n",
    "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
    "        # and we will get an error trying to concat these features\n",
    "        \n",
    "        \n",
    "        h, w = image.shape[2:]\n",
    "#         print(\"hello\",h,w)\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "\n",
    "        mask = batch[\"mask\"].to(torch.int64)\n",
    "        \n",
    "#         print(torch.unique(mask), mask.shape)\n",
    "        \n",
    "#         print(mask.shape, \"lll\")\n",
    "        \n",
    "        \n",
    "#         for i in range(1,49):\n",
    "#             mask[mask == i] = i\n",
    "            \n",
    "#         print\n",
    "\n",
    "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
    "        # for binary segmentation num_classes = 1\n",
    "#         assert mask.ndim == 4\n",
    "\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "#         assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "\n",
    "#         print(\"hello\", mask.max())\n",
    "        \n",
    "        \n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "        \n",
    "#         print(\"hello hello\", logits_mask.shape, mask.shape)\n",
    "        \n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Lets compute metrics for some threshold\n",
    "        # first convert mask values to probabilities, then \n",
    "        # apply thresholding\n",
    "        prob_mask = self.softmaxfunc(logits_mask)\n",
    "        pred_mask = torch.argmax(prob_mask, 1, keepdim=True)\n",
    "\n",
    "        # We will compute IoU metric by two ways\n",
    "        #   1. dataset-wise\n",
    "        #   2. image-wise\n",
    "        # but for now we just compute true positive, false positive, false negative and\n",
    "        # true negative 'pixels' for each image and class\n",
    "        # these values will be aggregated in the end of an epoch\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask[:,0,...].long(), mask.long(), \n",
    "                                               mode=\"multiclass\", num_classes=self.no_of_classes)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        \n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # per image IoU means that we first calculate IoU score for each image \n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset \n",
    "        # with \"empty\" images (images without target class) a large gap could be observed. \n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")            \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PetModel(\"DeepLabV3plus\", \"resnet50\",\"customnet\", in_channels=3, out_classes=49)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus = 1,\n",
    "    max_epochs=20,\n",
    "#     precision=16\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"./good_deeplabv3plus_final_scaled.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a39d0",
   "metadata": {},
   "source": [
    "# For Calculation of JaccardIndex on Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(\"./val_outputs/*/10.png\")) ## correct path to val_outputs change accordingly\n",
    "\n",
    "all_22_nd_validation_frames = sorted(glob.glob(\"./Dataset_Student/val_flat/*21.png\"))\n",
    "\n",
    "print(files[0], all_22_nd_validation_frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6651cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "for img_path, predicted_file_path in zip(all_22_nd_validation_frames, files):\n",
    "    \n",
    "    assert img_path.split(\"/\")[-1].split(\"_\")[1] == predicted_file_path.split(\"/\")[-2].split(\"_\")[1]\n",
    "    \n",
    "    bgr = cv2.imread(predicted_file_path)\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    l,a,b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0,tileGridSize=(10,10))\n",
    "\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    lab = cv2.merge((l,a,b))\n",
    "\n",
    "    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    cv2.imwrite(\"./val_rgb_files_fut_cv2/\" +img_path.split(\"/\")[-1], bgr)\n",
    "    \n",
    "    mask_path = \"Dataset_Student/val_targets_flat/\" +\"/\" + img_path.split(\"/\")[-1]\n",
    "    \n",
    "    print(mask_path,predicted_file_path,img_path)\n",
    "    \n",
    "    # Set the desired padding width\n",
    "    padding_width = 16\n",
    "\n",
    "    # Create a new image with the desired width and the same height as the original image\n",
    "    new_image = Image.new('RGB', (240 + padding_width, 160))\n",
    "\n",
    "    # Paste the original image onto the new image\n",
    "    new_image.paste(Image.open(\"./val_rgb_files_fut_cv2/\" +img_path.split(\"/\")[-1]), (0, 0))\n",
    "\n",
    "            \n",
    "    # Read image using PIL.Image and convert it to an RGB image\n",
    "    img = np.moveaxis(np.array(new_image) ,  -1, 0)\n",
    "\n",
    "\n",
    "#     img = np.load(predicted_file_path)\n",
    "#     img = np.pad(img, ((0, 0), (0, 16), (0, 0)), mode='constant')\n",
    "#     img = np.moveaxis(np.array(img), -1, 0)\n",
    "    \n",
    "#     img = np.clip(img,0,1)\n",
    "#     img = np.expand_dims(img,0)\n",
    "        \n",
    "        \n",
    "    # Set the desired padding width\n",
    "    mask_padding_width = 16\n",
    "\n",
    "    # Create a new image with the desired width and the same height as the original image\n",
    "    mask_new_image = Image.new('L', (240+mask_padding_width, 160))\n",
    "\n",
    "    # Paste the original image onto the new image\n",
    "    mask_new_image.paste(Image.open(mask_path), (0, 0))\n",
    "        \n",
    "    mask_img = np.array(mask_new_image)\n",
    "        \n",
    "    batched_file = dict(image=img, mask=mask_img)  \n",
    "    \n",
    "    batched_file[\"image\"] = torch.unsqueeze(torch.from_numpy(batched_file[\"image\"]),0)\n",
    "    batched_file[\"mask\"] = torch.unsqueeze(torch.from_numpy(batched_file[\"mask\"]),0)\n",
    "    \n",
    "#     print(batched_file[\"image\"].shape)\n",
    "#     print(batched_file[\"mask\"].shape)\n",
    "    \n",
    "    softmax_func = nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(batched_file[\"image\"])\n",
    "    pr_masks = softmax_func(logits)\n",
    "    pr_masks = torch.argmax(pr_masks, 1, keepdim=True)\n",
    "    \n",
    "#     print(batch[\"image\"][0].shape)\n",
    "#     print(batch[\"mask\"][0].dtype)\n",
    "#     print(pr_masks[0].dtype)\n",
    "    \n",
    "    \n",
    "    im = Image.fromarray(batched_file[\"image\"][0].numpy().transpose(1, 2, 0)[:,:240,:])\n",
    "    im.save(\"val_rgb_files_fut/\"+ img_path.split(\"/\")[-1])\n",
    "    \n",
    "    im = Image.fromarray(batched_file[\"mask\"][0].numpy().squeeze()[:,:240])\n",
    "    im.save(\"val_segmentation_gts_fut/\"+ img_path.split(\"/\")[-1])\n",
    "    \n",
    "    im = Image.fromarray(pr_masks[0].numpy().squeeze().astype('uint8')[:,:240])\n",
    "    im.save(\"val_pred_segs_fut/\"+ img_path.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32841672",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_files = sorted(glob.glob(\"./val_segmentation_gts_fut/*\"))\n",
    "pred_files = sorted(glob.glob(\"./val_pred_segs_fut/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81edc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "gt_tensor_list = []\n",
    "pred_tensor_list = []\n",
    "\n",
    "for at_file, pred_file in zip(actual_files, pred_files):\n",
    "    \n",
    "    print(at_file,pred_file)\n",
    "    \n",
    "    at_image = Image.open(at_file)\n",
    "\n",
    "    # Convert the image data into a NumPy array\n",
    "    at_image_array = np.array(at_image)\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    at_image_tensor = torch.from_numpy(at_image_array)\n",
    "    \n",
    "    pred_image = Image.open(pred_file)\n",
    "\n",
    "    # Convert the image data into a NumPy array\n",
    "    pred_image_array = np.array(pred_image)\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    pred_image_tensor = torch.from_numpy(pred_image_array)\n",
    "    \n",
    "    \n",
    "    gt_tensor_list.append(at_image_tensor)\n",
    "    pred_tensor_list.append(pred_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed958343",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_tensor_stacked = torch.stack(gt_tensor_list, dim=0)\n",
    "pred_tensor_stacked = torch.stack(pred_tensor_list, dim=0)\n",
    "\n",
    "print(gt_tensor_stacked.shape)\n",
    "print(pred_tensor_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0193c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gt_tensor_stacked, './gt_tensor_stacked.pth')\n",
    "torch.save(pred_tensor_stacked, './pred_tensor_stacked.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7c64a",
   "metadata": {},
   "source": [
    "# For Generating 22nd segmentation frame for Hidden Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(\"./HiddenPreds/*/10.png\")) # path to where the hidden predictions are generated\n",
    "\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "for predicted_file_path in files:\n",
    "    \n",
    "#     assert img_path.split(\"/\")[-1].split(\"_\")[1] == predicted_file_path.split(\"/\")[-2].split(\"_\")[1]\n",
    "    \n",
    "    bgr = cv2.imread(predicted_file_path)\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    l,a,b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0,tileGridSize=(10,10))\n",
    "\n",
    "    l = clahe.apply(l)\n",
    "\n",
    "    lab = cv2.merge((l,a,b))\n",
    "\n",
    "    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    cv2.imwrite(\"./val_rgb_files_fut_cv2/\"+ predicted_file_path.split(\"/\")[-2]+ \"_\" +predicted_file_path.split(\"/\")[-1], bgr)\n",
    "    \n",
    "#     mask_path = \"Dataset_Student/val_targets_flat/\" +\"/\" + img_path.split(\"/\")[-1]\n",
    "    \n",
    "    print(predicted_file_path,\"./val_rgb_files_fut_cv2/\"+ predicted_file_path.split(\"/\")[-2] + \"_\" + predicted_file_path.split(\"/\")[-1])\n",
    "    \n",
    "    # Set the desired padding width\n",
    "    padding_width = 16\n",
    "\n",
    "    # Create a new image with the desired width and the same height as the original image\n",
    "    new_image = Image.new('RGB', (240 + padding_width, 160))\n",
    "\n",
    "    # Paste the original image onto the new image\n",
    "    new_image.paste(Image.open(\"./val_rgb_files_fut_cv2/\"+ predicted_file_path.split(\"/\")[-2] + \"_\" +predicted_file_path.split(\"/\")[-1]), (0, 0))\n",
    "\n",
    "            \n",
    "    # Read image using PIL.Image and convert it to an RGB image\n",
    "    img = np.moveaxis(np.array(new_image) ,  -1, 0)\n",
    "\n",
    "\n",
    "#     img = np.load(predicted_file_path)\n",
    "#     img = np.pad(img, ((0, 0), (0, 16), (0, 0)), mode='constant')\n",
    "#     img = np.moveaxis(np.array(img), -1, 0)\n",
    "    \n",
    "#     img = np.clip(img,0,1)\n",
    "#     img = np.expand_dims(img,0)\n",
    "        \n",
    "        \n",
    "    # Set the desired padding width\n",
    "    mask_padding_width = 16\n",
    "\n",
    "    # Create a new image with the desired width and the same height as the original image\n",
    "#     mask_new_image = Image.new('L', (240+mask_padding_width, 160))\n",
    "\n",
    "#     # Paste the original image onto the new image\n",
    "#     mask_new_image.paste(Image.open(mask_path), (0, 0))\n",
    "        \n",
    "#     mask_img = np.array(mask_new_image)\n",
    "        \n",
    "    batched_file = dict(image=img)  \n",
    "    \n",
    "    batched_file[\"image\"] = torch.unsqueeze(torch.from_numpy(img),0)\n",
    "#     batched_file[\"mask\"] = torch.unsqueeze(torch.from_numpy(batched_file[\"mask\"]),0)\n",
    "    \n",
    "#     print(batched_file[\"image\"].shape)\n",
    "#     print(batched_file[\"mask\"].shape)\n",
    "    \n",
    "    softmax_func = nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(batched_file[\"image\"])\n",
    "    pr_masks = softmax_func(logits)\n",
    "    pr_masks = torch.argmax(pr_masks, 1, keepdim=True)\n",
    "    \n",
    "#     print(batch[\"image\"][0].shape)\n",
    "#     print(batch[\"mask\"][0].dtype)\n",
    "#     print(pr_masks[0].dtype)\n",
    "    \n",
    "    \n",
    "    im = Image.fromarray(batched_file[\"image\"][0].numpy().transpose(1, 2, 0)[:,:240,:])\n",
    "    im.save(\"val_rgb_files_fut/\"+  predicted_file_path.split(\"/\")[-2] + \"_\"+ predicted_file_path.split(\"/\")[-1])\n",
    "    \n",
    "#     im = Image.fromarray(batched_file[\"mask\"][0].numpy().squeeze()[:,:240])\n",
    "#     im.save(\"val_segmentation_gts_fut/\"+ img_path.split(\"/\")[-1])\n",
    "    \n",
    "    im = Image.fromarray(pr_masks[0].numpy().squeeze().astype('uint8')[:,:240])\n",
    "    im.save(\"val_pred_segs_fut/\"+ predicted_file_path.split(\"/\")[-2] + \"_\"+ predicted_file_path.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_files = sorted(glob.glob(\"./val_pred_segs_fut/*\"))\n",
    "\n",
    "print(pred_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "pred_tensor_list = []\n",
    "\n",
    "for  pred_file in pred_files:\n",
    "    \n",
    "    print(pred_file)\n",
    "    \n",
    "    \n",
    "    pred_image = Image.open(pred_file)\n",
    "\n",
    "    # Convert the image data into a NumPy array\n",
    "    pred_image_array = np.array(pred_image)\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    pred_image_tensor = torch.from_numpy(pred_image_array)\n",
    "    \n",
    "    \n",
    "    pred_tensor_list.append(pred_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tensor_stacked = torch.stack(pred_tensor_list, dim=0)\n",
    "\n",
    "print(pred_tensor_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pred_tensor_stacked, './hidden_set_predicted_masks.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
